{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import random\n",
    "import math\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_num_threads = 12\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(implicit_num_threads)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(implicit_num_threads)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(implicit_num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gomoku_game:\n",
    "    def __init__(self):\n",
    "        self.dim = 15 #15x15 board, can be 19x19\n",
    "        self.state = np.zeros([self.dim, self.dim], dtype=np.int8)\n",
    "        self.players = {'p1': 1, 'p2': 2} \n",
    "        self.isDone = False\n",
    "        self.reward = {'win': 1, 'draw': 0.5, 'loss': -1}\n",
    "        self.available = [] #initialize to all spots on board\n",
    "        for i in range(self.dim):\n",
    "            for j in range(self.dim):\n",
    "                self.available.append(i + j* self.dim)\n",
    "    \n",
    "    def render(self): #rev\n",
    "        rendered_board_state = self.state.copy().astype(str)\n",
    "        rendered_board_state[self.state == 0] = ' '\n",
    "        rendered_board_state[self.state == 1] = 'B'\n",
    "        rendered_board_state[self.state == 2] = 'W'\n",
    "        display(pd.DataFrame(rendered_board_state))\n",
    "    \n",
    "    def reset(self): #rev\n",
    "        self.__init__()\n",
    "        \n",
    "    def get_available_actions(self):\n",
    "        return self.available\n",
    "    \n",
    "\n",
    "    def check_game_done(self, player, last):\n",
    "        def check_flat(state, i, j):\n",
    "            ci = i\n",
    "            while ci > 0 and state[ci][j] == state[i][j]: \n",
    "                ci -= 1\n",
    "            if state[ci][j] != state[i][j]:\n",
    "                left = ci + 1\n",
    "            else:\n",
    "                left = ci\n",
    "            ri = i\n",
    "            while ri < (len(state) - 1) and state[ri][j] == state[i][j]: \n",
    "                ri += 1 \n",
    "            if state[ri][j] != state[i][j]:\n",
    "                right = ri - 1\n",
    "            else:\n",
    "                right = ri\n",
    "            if right - left == 4: \n",
    "                return True \n",
    "            return False\n",
    "\n",
    "        '''\n",
    "        find length of current diag with equal values and if over 4 return true \n",
    "        '''\n",
    "        def check_diag(state, i, j):\n",
    "            li = i\n",
    "            lj = j\n",
    "            while li > 0 and lj > 0 and state[li][lj] == state[i][j]:\n",
    "                li -= 1\n",
    "                lj -= 1\n",
    "            if state[li][lj] != state[i][j]:\n",
    "                lowi = li + 1\n",
    "                lowj = lj + 1\n",
    "            else:\n",
    "                lowi = li\n",
    "                lowj = lj\n",
    "\n",
    "            ui = i\n",
    "            uj = j\n",
    "            while ui < len(state) - 1 and uj < len(state) - 1 and state[ui][uj] == state[i][j]:\n",
    "                ui += 1\n",
    "                uj += 1\n",
    "            if state[ui][uj] != state[i][j]:\n",
    "                upi = ui - 1\n",
    "                upj = uj - 1\n",
    "            else:\n",
    "                upi = ui\n",
    "                upj = uj\n",
    "\n",
    "            if upi - lowi == 4: \n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        '''\n",
    "        input:\n",
    "            state: board state (15x15 numpy array)\n",
    "        output:\n",
    "            win: boolean value of whether current player won\n",
    "        '''\n",
    "        \n",
    "        #check diagonal\n",
    "        #check horizontal\n",
    "        #check vertical\n",
    "        i, j = last\n",
    "        stateCp = self.state.copy()\n",
    "\n",
    "        vert = check_flat(stateCp, i, j)  #I dont believe these need to be copied\n",
    "        horiz = check_flat(stateCp.T, j, i)\n",
    "        \n",
    "        # print(\"hor\" + str(horiz))\n",
    "        # print(\"vert\" + str(vert))\n",
    "        \n",
    "        #i think this should work??\n",
    "        diagl = check_diag(stateCp, i, j)\n",
    "        diagr = check_diag(np.fliplr(stateCp), i, len(self.state) -1 - j)\n",
    "\n",
    "        diag = diagl or diagr\n",
    "\n",
    "        return (horiz or vert) or diag\n",
    "\n",
    "\n",
    "    def make_move(self, a, player):\n",
    "        # check if move is valid\n",
    "        i = a % self.dim\n",
    "        j = int((a - i)/ self.dim)\n",
    "        openSpots = self.get_available_actions()\n",
    "        if a in openSpots:\n",
    "            self.state[i,j] = self.players[player]\n",
    "            self.available.remove(a)\n",
    "        else:\n",
    "            print('Move is invalid')\n",
    "            self.render()\n",
    "        a = (i, j)\n",
    "        win = self.check_game_done(player, a)\n",
    "\n",
    "        reward = 0.\n",
    "        if len(openSpots) == 0:\n",
    "            reward = self.reward['draw']\n",
    "            self.isDone = True\n",
    "        elif win:  \n",
    "            reward = self.reward['win']\n",
    "            self.isDone = True\n",
    "\n",
    "        # give feedback as new state and reward\n",
    "        return self.state.copy(), reward\n",
    "\n",
    "env = gomoku_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "\n",
    "#***based on torch tutorial\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = deque([])\n",
    "\n",
    "    #could use *args for historical turns? \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        # 6 by 7, 10 by 11 \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(7,7), padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(7,7), padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(7,7), padding=2)\n",
    "        # self.conv4 = nn.Conv2d(128, 128, kernel_size=4, padding=2)\n",
    "        # self.conv5 = nn.Conv2d(128, 64, kernel_size=4, padding=2)\n",
    "        # self.conv6 = nn.Conv2d(64, 32, kernel_size=4, padding=2)\n",
    "        # self.conv7 = nn.Conv2d(32, 32, kernel_size=4, padding=2)\n",
    "\n",
    "        # linear_input_size = 6 * 7 * 32\n",
    "        self.MLP1 = nn.Linear(2592, 1024)\n",
    "        self.MLP2 = nn.Linear(1024, 1024)\n",
    "        # self.MLP3 = nn.Linear(50, 50)\n",
    "        self.MLP4 = nn.Linear(1024, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        # x = F.leaky_relu(self.conv4(x))\n",
    "        # x = F.leaky_relu(self.conv5(x))\n",
    "        # x = F.leaky_relu(self.conv6(x))\n",
    "        # x = F.leaky_relu(self.conv7(x))\n",
    "        # flatten the feature vector except batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.MLP1(x))\n",
    "        x = F.leaky_relu(self.MLP2(x))\n",
    "        # x = F.leaky_relu(self.MLP3(x))\n",
    "        return self.MLP4(x)\n",
    "    \n",
    "\n",
    "# def DQN (outputs):\n",
    "#     model = nn.Sequential(\n",
    "#         nn.Conv2d(1, 32, kernel_size=(7,7), padding=2), \n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(32, 32, kernel_size=(7,7), padding=2),\n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.ReLU(),\n",
    "#         # nn.MaxPool2d((2,2)),\n",
    "#         nn.Conv2d(32, 64, kernel_size=(7,7), padding=2),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU(),\n",
    "#         # nn.MaxPool2d((2,2)),\n",
    "#         nn.Flatten(),\n",
    "#         nn.Linear(5184, 1024),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(1024, outputs),\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "B = 256 #batch size\n",
    "gamma = 0.99 #0.999 or 0.9999 or decrease?\n",
    "lr = 1e-4\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 2000\n",
    "tau = 0.005\n",
    "\n",
    "bSize = env.dim #board dimensions\n",
    "n_actions = bSize **2\n",
    "\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "target_net.eval() # set target net into eval mode\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "#STEPS DONE - Maybe isolate this variable so dont always need to reset eps?\n",
    "steps_done = 0\n",
    "training_history = []\n",
    "\n",
    "def select_action(state, available_actions, training=True):\n",
    "    current_chance = random.random()\n",
    "    \n",
    "    #available_actions = state.get_available_actions()\n",
    "    if training:\n",
    "        global steps_done\n",
    "        eps_threshold = eps_start + (eps_start - eps_end) * math.exp(-1 * steps_done / eps_decay)\n",
    "    else:\n",
    "        eps_threshold = 0\n",
    "\n",
    "    if current_chance > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            all_probabilities = policy_net(torch.tensor(state, dtype=torch.float, device=device).unsqueeze(dim=0).unsqueeze(dim=0))[0, :] #unsqez\n",
    "            potential_action_probs = [all_probabilities[a] for a in available_actions]\n",
    "            return available_actions[np.argmax(potential_action_probs)] #Always pick most likely - could use an additional layer of randomness for fun tho?\n",
    "    else: \n",
    "        return random.choice(available_actions)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#episode_duration = []\n",
    "\n",
    "#def plot duractions -> slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization of model \n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < B:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(B)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # state_batch, action_batch, reward_batch, next_state_batch = zip(*[(np.expand_dims(m[0], axis=0), \\\n",
    "    #                                     [m[1]], m[2], np.expand_dims(m[3], axis=0)) for m in transitions])\n",
    "    # print(batch.state)\n",
    "    # alt_state = np.array((len(batch.state), 15, 15))\n",
    "    # for i, v in enumerate(batch.state):\n",
    "    #     print(type(v))\n",
    "    #     alt_state[i]= v\n",
    "\n",
    "\n",
    "    state_batch = torch.tensor(batch.state, dtype=torch.float, device=device)\n",
    "    action_batch = torch.tensor(batch.action, dtype=torch.long, device=device)\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.float, device=device)\n",
    "\n",
    "    # print(len(state_batch))\n",
    "    # print(type(state_batch))\n",
    "    # state_batch = torch.from_numpy(state_batch)\n",
    "    # action_batch = torch.from_numpy(action_batch)\n",
    "    # reward_batch = torch.from_numpy(reward_batch)\n",
    "\n",
    "    # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "    #                                       batch.next_state)), device=device)\n",
    "    # non_final_next_states = torch.cat([torch.tensor(s, dtype=torch.float, device=device).unsqueeze(0) for s in batch.next_state\n",
    "    #                                             if s is not None])\n",
    "    \n",
    "    # non_final_next_state = torch.cat([torch.tensor(s_, dtype=torch.float, device=device).unsqueeze(0) for s_ in next_state_batch if s_[0] is not None])\n",
    "\n",
    "\n",
    "\n",
    "    # state_batch = torch.cat(torch.from_numpy((batch.state)[0]))\n",
    "    # action_batch = torch.cat(batch.action)\n",
    "    # reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "    non_final_next_states = torch.cat([torch.tensor(s_, dtype=torch.float, device=device).unsqueeze(0) for s_ in batch.next_state if s_ is not None])\n",
    "\n",
    "    # print(.shape)\n",
    "    state_action_values = policy_net(state_batch.unsqueeze(1)).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "    next_state_values = torch.zeros(B, device=device)\n",
    "    \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states.unsqueeze(1)).max(1)[0].detach()\n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "def random_agent(actions):\n",
    "    return random.choice(actions)\n",
    "\n",
    "# win rate test\n",
    "def win_rate_test():\n",
    "    win_moves_taken_list = []\n",
    "    win = []\n",
    "    for i in range(100):\n",
    "        env.reset()\n",
    "        win_moves_taken = 0\n",
    "\n",
    "        while not env.isDone:\n",
    "            state = env.state.copy()\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = select_action(state, available_actions, training=False)\n",
    "            state, reward = env.make_move(action, 'p1')\n",
    "            win_moves_taken += 1\n",
    "\n",
    "            if reward == 1:\n",
    "                win_moves_taken_list.append(win_moves_taken)\n",
    "                win.append(1)\n",
    "                break\n",
    "\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = random_agent(available_actions)\n",
    "            state, reward = env.make_move(action, 'p2')\n",
    "\n",
    "    return sum(win)/100, sum(win_moves_taken_list)/len(win_moves_taken_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "num_episodes = 200\n",
    "# control how lagged is target network by updating every n episodes\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "for i in range(num_episodes): \n",
    "    env.reset()\n",
    "    state_p1 = env.state.copy()\n",
    "    # print(i)\n",
    "    # record every 20 epochs\n",
    "    print(i)\n",
    "    if i % 20 == 19:\n",
    "        win_rate, moves_taken = win_rate_test()\n",
    "        training_history.append([i + 1, win_rate, moves_taken])\n",
    "        th = np.array(training_history)\n",
    "        # print training message every 200 epochs\n",
    "        if i % 20 == 19:\n",
    "            print('Episode {}: | win_rate: {} | moves_taken: {}'.format(i, th[-1, 1], th[-1, 2]))\n",
    "\n",
    "    for t in count():\n",
    "        # print(\"two\")\n",
    "        available_actions = env.get_available_actions()\n",
    "        action_p1 = select_action(state_p1, available_actions, steps_done)\n",
    "        steps_done += 1\n",
    "        state_p1_, reward_p1 = env.make_move(action_p1, 'p1')\n",
    "        \n",
    "        if env.isDone:\n",
    "            if reward_p1 == 1:\n",
    "                # reward p1 for p1's win\n",
    "                memory.push(state_p1, action_p1, 1, None)\n",
    "            else:\n",
    "                # state action value tuple for a draw\n",
    "                memory.push(state_p1, action_p1, 0.5, None)\n",
    "            break\n",
    "        \n",
    "        available_actions = env.get_available_actions()\n",
    "        action_p2 = random_agent(available_actions)\n",
    "        state_p2_, reward_p2 = env.make_move(action_p2, 'p2')\n",
    "        \n",
    "        if env.isDone:\n",
    "            if reward_p2 == 1:\n",
    "                # punish p1 for (random agent) p2's win \n",
    "                memory.push(state_p1, action_p1, -1, None)\n",
    "            else:\n",
    "                # state action value tuple for a draw\n",
    "                memory.push(state_p1, action_p1, 0.5, None)\n",
    "            break\n",
    "        \n",
    "        # punish for taking too long to win\n",
    "        memory.push(state_p1, action_p1, -0.05, state_p2_)\n",
    "        state_p1 = state_p2_\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        \n",
    "    # update the target network, copying all weights and biases in DQN\n",
    "    if i % TARGET_UPDATE == TARGET_UPDATE - 1:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
